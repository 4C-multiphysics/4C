\chapter{Binary IO}

This section was originally written to be both: An indepth presentation of the
new binary IO machinery in \ccarat{} and an advertisement for
it. Unfortunately it does not explain how to add new elements. For that you
will have to read the extensive code comments.

Note: This discussion concerns the C style binary IO implemented in
\ccarat{}. IO in \baci{} follows the same lines, but turns out to be much
simpler to use and extend.

\section{The purpose of output}

To be of use ccarat needs the ability to write the calculated values.
That's obvious. The interesting values live in various places.


\subsection{Node arrays}

Of course there are the node arrays. These arrays are always there,
but their sizes and the meaning of the data inside them vary a lot.
Basically each algorithm has its own idea what's going on in there.
Only the elements constrain the algorithms' freedom in assigning meaning
to the node arrays because the elements, too, rely on these arrays.

Most of the time the node arrays have as many columns as there are
dofs at this node and as many rows as required by the algorithm. Only
a few of them are interessting results. The others are just working
data.


\subsection{Element data}

The second source of values that need to be written somewhere are
the elements themselves. Stresses, for example, are calculated at
the element level therefore stored in element arrays. But element
data is highly element specific. Each element is different.


\subsection{Distributed Vectors}

A third, less obvious, source of data are the distributed vectors
that make up our rhs and solution vectors. These need to be stored
for restart purposes. One would not attempt to postprocess them, but
the io system must support them nevertheless.


\section{Output targets}

The output that ccarat produces is used in many different ways.


\subsection{Restart}

For restart it is necessary to save the current step's working data.
There is no need to store any input, the input file will be read again
anyway. There is no need to store data that can easily be reconstructed,
too. Thus we don't store the current system matrix. But we need to
store all node arrays and everything that cannot be reconstructed
without recalculating former steps.

The reading part of our restart facility builds on the assumption
that all data structures are set up by the algorithm. No memory allocation
is done by the io module. It only reads the data files and puts the
values to the right places in memory.

The restart system is not concerned with the meaning of the data it
reads and writes.


\subsection{GiD}

Talking about GiD is talking about postprocessing. We need to give
all information to GiD that's needed to visualize the results. In
particular the node coordinates and mesh connectivity must be written
as well, because GiD never sees the ccarat input file. (Okay, that's
a lie. These files are produced by GiD. But that's another story.)

But looking closer one sees that ccarat's view of its data is different
from what GiD, or any other postprocessor for that matter, expects.
So a slight transformation has to be applied.


\subsection{Plain Text}

Often people (read: developer) are interessted in the real numbers.
So some means to output a plain text file are necessary. Transformations
are not welcome here. The plain text output has to show the real ccarat
data structures.


\subsection{Monitor}

Sometimes only some selected dofs are important. We want to be able
to watch these dofs, that is write their evolution to a separate file.


\subsection{Visual}

There are the visual libraries that allow to visualize 2d and 3d fluid
results. (That's nothing but a special postprocessor.)


\section{State of the art}

The old way of handling output (which is still the current way, but
things are going to change) is to handle every special case separately.
There are {*}.pss files that are used for restart (These are binary
files, the only ones that are binary. But these files are very closely
linked to ccarat's internal data structures.) There are those flavia
files GiD asks for. There are {*}.out files that contain a nice readable
representation and so on. The monitor facility is build into ccarat
and it's even possible to link ccarat with the visual libraries.

In general the situation is not too bad, after all output works fine
and people are free to think about the important stuff. Sure, the
code contains lots of redundancies (this is especially true for the
GiD output part) but it's straight forward to understand and you'll
have little difficulty to add you own element. This is what actually
happend quite some times already. It has grown a lot by copy and paste.

In a parallel setting all processes write some files. Each process
writes its own data to its binary (pss) file. The text output (GiD),
however, is done by the first process only. Thus all results must
be available there.


\section{New Requirements}

But our attempt to improve ccarat's parallelization enforces a different
style of output. We cannot write our results to plain text files anymore.
And we don't want to constrain our output files to the number of processes
that were used to generate them. Thus neither of the current approaches
fulfills our needs. Additionally we want to be plattform independent
(we want to use the same format on little endian and big endian plattforms).

Finally we have to recognize that ccarat's file format is part of
its interface, this means it must be clean and simple and must not
expose internal details. This files will be read by external programs
and, even more frightening, they will stay around for a while. When
we calculate big problems we'll keep the results. Therefore we have
to maintain ccarat's ability to read it's old files.

Of course the most important old requirement does remain: Our output
system must be as simple to enhance as possible. But nowadays we accept
a certain complexity for the sake of efficiency.


\section{The new approach}

We want to unify the various output methods, we want ccarat to have
one way to output data. All data is supposed to go into the same file.
Of course, it's going to be a binary one. This file can be postprocessed
by various filters afterwards. These filters are responsible to apply
any required transformation and to create the input needed for your
favourite postprocessing tool.

Actually just one file will not do. In ccarat the central data structures
are the discretizations. Each discretization has its own set of nodes
and elements. We'll have to have a per discretization output. Thus
it seems wise to put each discretization in a different output file.

Looking closer we find two kinds of data we have to write: integers
and doubles. If we manage to separate these and put them to different
files, both files will have a very simple structure. The big advantage
is that we can easily change the endianess of both of them and we
can, if everything else fails, look at these files with \texttt{hexdump}
or similar tools. So we are going to have two files per discretization.

This way the bulk data can be stored, but we still have to keep track
of what we write to these files. We'll have to store a description
somewhere. To do this we're going to write an additional control file.
No matter how many discretizations or processes there are, we just
need one control file for each ccarat run. This file is supposed to
contain very little compared to the two value files. Thus control
files can be plain text. That's convenient because this way one can
easily read it and see that kind of data has been written.


\section{File format}

During a ccarat run there are lots of results to be stored. A nonlinear
shell problem, for example, might require to store displacements and
stresses in each interation. That is there are six displacement values
per node and six times ngauss stress values per element. Any other
example will have a different number of values with different meaning
to be written, however the two cases, nodal values or element values,
are typical. The general picture is that for every time step for every
result there's set of items (nodes or elements) each contributing
some values to be written. Such a collection of values is called chunk.
A chunk consists of consecutive entries of constant size. Each entry
contains the contribution of one node or element. The entries inside
a chunk are ordered by their items' ids. There are many chunks in
the output files, one after the other.

Each chunk comes with a group definition in the control file. Such
a group tells where the chunk starts (offset in bytes) in the double
and integer data files as well as the length of its entries (in double
or integer units). Additionally it contains the information whether
this is a node or element chunk, even though that's redundant, the
name of the group already conveys the meaning of the corresponding
chunk.

Take the following \texttt{mesh} group as an example. It describes
a chunk that contains the elements' ids, their types and the ids of
those nodes an element is connected to. There is a \texttt{mesh} group
in each discretizations main group: 

\begin{quote}
\texttt{mesh:~}~\\
 \texttt{~~~~size{\_}entry{\_}length~=~7~}~\\
 \texttt{~~~~size{\_}offset~=~0~}~\\
 \texttt{~~~~type~=~\char`\"{}element\char`\"{}~}~\\
 \texttt{~~~~value{\_}entry{\_}length~=~0~}~\\
 \texttt{~~~~value{\_}offset~=~0 }
\end{quote}
As you see both \texttt{size{\_}offset} and \texttt{value{\_}offset}
are zero. That is the chunk starts at the beginning of both data files,
where \texttt{value} always refers to the file containing double values.
The integer file is called 'size' file because the first values it
ever contained were the node array's dimensions (and because a name
was needed). \texttt{size{\_}entry{\_}length} is seven, that is
there are seven integer values per element. Accordingly \texttt{value{\_}entry{\_}length
= 0} means that this chunk contains no double values.

Please note that the number of nodes and elements per discretization
are known (there is the \texttt{numnp} and the \texttt{numele} variable
in the discretizations main group in the control file) and that the
total size of one chunk can be determined with this information.

There is another group that has to be written for each discretization.
It's called \texttt{coords} and contains the node coordinates. Here
is an example: 

\begin{quote}
\texttt{coords:~}~\\
 \texttt{~~~~size{\_}entry{\_}length~=~1~}~\\
 \texttt{~~~~size{\_}offset~=~28672~}~\\
 \texttt{~~~~type~=~\char`\"{}node\char`\"{}~}~\\
 \texttt{~~~~value{\_}entry{\_}length~=~2~}~\\
 \texttt{~~~~value{\_}offset~=~0 }
\end{quote}
The size file contains one integer per node. It's the node's (global)
id. The value file contains the coordinate values. The above example
belongs to a 2d problem with two coordinate values per node.

Generally, the control file consists of a sequence of definitions
and have a pythonic flavor. There are simple definitions of the form: 

\begin{quote}
\texttt{attr{\_}name~=~value }
\end{quote}
where \texttt{attr{\_}name} is any name consisting of letters, digits
and underscores. \texttt{value} might be a string, an integer or a
double value. Additionally there are group definitions like the ones
above. Groups always start with the group's name and a colon. The
definitions a group contains are recognized by their indention. Groups
can be nested.

A particular thing is that names (in one group) need not be unique.
All values that are assigned to one name will be remembered. This
feature can be used to have many \texttt{result} groups, one for each
time step, for example. For simple variable definitions (like the
\texttt{numnp} and \texttt{numele} variables mentioned above) the
feature is of no use. In these cases the \textbf{last} definition
counts.

The control file reader and the internal representation it generates
(a map hierarchy) are defined in the file \texttt{src/pss{\_}full/pss{\_}table.c}.
It might be usable in other contexts, too.


\section{Implementation strategy}

For efficiency reasons we want to write in parallel, utilizing the
magic of MPI IO. Our io module has to handle problems that are too
big to fit in the memory of one processing unit. On the other hand,
however, we want to save the items of one chunk sorted by its id.
To do this we assign each processor a consecutive range of ids it
is responsible for. The first processor gets the first range, the
second the second and so on. This way we can determine very easily
which processor is responsible for a particular item.

This scheme will not coincide with the physical reasonable distribution
figured out by metis. So we need a communication layer that sends
the values to be stored from the processor where its node or element
lives to the processor that's responsible for writing it. The same
goes for the reading part (just in the opposite direction).

The details how that's done are discussed further down below. We just
need the general picture in order to understand the following.


\section{How to use the io module}


\subsection{Example: Binary io in fluids}

To see a real working example of the binary output lets have a look
at the fluid algorithm \texttt{fluid{\_}isi} in \texttt{fluid{\_}imp{\_}semimp.c}.
This file includes \texttt{io.h} from the io module which makes the
io definitions available. When the function \texttt{fluid{\_}isi}
is called the general io module's setup has already been done, but
some discretization specific setup is needed as well. In order to
have this we need an output variable: 

\begin{quote}
\texttt{BIN{\_}OUT{\_}FIELD~~~out{\_}context; }
\end{quote}
It is used to call the output initialization: 

\begin{quote}
\texttt{init{\_}bin{\_}out{\_}field({\&}out{\_}context,~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~~{\&}(actsolv->sysarray{\_}typ{[}actsysarray]),~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~~{\&}(actsolv->sysarray{[}actsysarray]),~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~~actfield,~actpart,~actintra,~0); }
\end{quote}
At this point in time the discretization's data files are open and
the connectivity (see above) has already be written. Now there is
the calculation going on. It's a parallel calculation. Longish and
difficult. Better have restart information written every now and then: 

\begin{quote}
\texttt{restart{\_}write{\_}bin{\_}fluiddyn({\&}out{\_}context,fdyn); }
\end{quote}
Of course some postprocessing output is needed as well: 

\begin{quote}
\texttt{if~(ioflags.fluid{\_}sol{\_}gid==1)~{\{}~}~\\
 \texttt{~~out{\_}results({\&}out{\_}context,~fdyn->acttime,~fdyn->step,~actpos,~}~\\
 \texttt{~~~~~~~~~~~~~~OUTPUT{\_}VELOCITY~|~OUTPUT{\_}PRESSURE);~}~\\
 \texttt{{\}}~}~\\
 \texttt{if~(ioflags.fluid{\_}stress{\_}gid==1)~{\{}~}~\\
 \texttt{~~out{\_}results({\&}out{\_}context,~fdyn->acttime,~fdyn->step,~actpos,~}~\\
 \texttt{~~~~~~~~~~~~~~OUTPUT{\_}STRESS);~}~\\
 \texttt{{\}} }
\end{quote}
When everything is said and done we destroy the context: 

\begin{quote}
\texttt{destroy{\_}bin{\_}out{\_}field({\&}out{\_}context); }
\end{quote}
That's it.

But yes, that's just the calling. You might want to enhance the io
system. To do this a closer look is needed.


\section{A closer look: Restart}

In the io module you can find the file \texttt{io{\_}restart.c}
that provides the two functions to write and read all information
needed to restart a simple fluid calculation.


\subsection{Writing}

The function \texttt{restart{\_}write{\_}bin{\_}fluiddyn} that
writes the restart data consists basically of three parts. The first
one writes the general information to the control file: 

\begin{quote}
\texttt{if~(rank~==~0)~{\{}~}~\\
 \texttt{~~out{\_}main{\_}group{\_}head(context,~\char`\"{}restart\char`\"{});~}~\\
 \texttt{~~fprintf(bin{\_}out{\_}main.control{\_}file,~}~\\
 \texttt{~~~~~~~~~~\char`\"{}~~~~step~=~{\%}d{\textbackslash{}}n\char`\"{}~}~\\
 \texttt{~~~~~~~~~~\char`\"{}~~~~time~=~{\%}20.20f{\textbackslash{}}n\char`\"{}~}~\\
 \texttt{~~~~~~~~~~\char`\"{}{\textbackslash{}}n\char`\"{},~}~\\
 \texttt{~~~~~~~~~~fdyn->step,~fdyn->acttime);~}~\\
 \texttt{{\}} }
\end{quote}
This is done on the first processor only. A new group named \texttt{restart}
is started, that knows its field, discretization number, and step
(and is thus unique) as well as the current time. (There is one more
variable: the field position. That's an unique number to each field.
It's needed because there might be problems with many fields of the
same type.)

Please note that this function relies on a proper setup of the io
module. The \texttt{bin{\_}out{\_}main} variable must be initialized.
The first parameter of the function, a pointer to a \texttt{struct
{\_}BIN{\_}OUT{\_}FIELD}, is something that needs to be specified,
too. We'll talk about this later.

The next part just writes the node arrays we care about: 

\begin{quote}
\texttt{out{\_}node{\_}arrays(context,~node{\_}array{\_}sol);~}~\\
 \texttt{out{\_}node{\_}arrays(context,~node{\_}array{\_}sol{\_}increment);~}~\\
 \texttt{if~(context->max{\_}size{[}node{\_}array{\_}sol{\_}mf]~>~0)~{\{}~}~\\
 \texttt{~~out{\_}node{\_}arrays(context,~node{\_}array{\_}sol{\_}mf);~}~\\
 \texttt{{\}} }
\end{quote}
Here the \texttt{out{\_}node{\_}arrays} function does all the
work. We just need to specify which node array is to be written. The
\texttt{sol{\_}mf} array is optional (it's used in multifield problems
only) and won't be read later if we don't write it now. Internally
this function will gather the data, write it and add a new subgroup
to the control file to describe the chunk of data it has written.

The third part consists of two simple function calls that write all
restart related element data: 

\begin{quote}
\texttt{find{\_}restart{\_}item{\_}length(context,~{\&}value{\_}length,~{\&}size{\_}length);~}~\\
 \texttt{out{\_}element{\_}chunk(context,~\char`\"{}element{\_}data\char`\"{},~cc{\_}restart{\_}element,~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~value{\_}length,~size{\_}length,~0); }
\end{quote}
Yet these innocent looking calls are very haunting. It's highly element
specific what kind of data we need to write. Thus both \texttt{find{\_}restart{\_}item{\_}length},
that has to find the length of one chunk entry, as well as \texttt{out{\_}element{\_}chunk},
the function that gathers the data and writes the chunk, have to contain
element specific code and need to be changed when the elements evolve.

The first function is comparatively simple. In essence it loops the
discretization, finds out what elements there are and sets the output
parameters accordingly. The function \texttt{out{\_}element{\_}chunk},
however, is a different beast. It does all the work and does it in
a generic way (the same functions are used for element and nodal chunks).
That's not something every user can be forced to study closely, for
that reason all element specific code is collected in a subfunction
called \texttt{out{\_}pack{\_}items} (defined in \texttt{io{\_}packing.c}).
This function is responsible to gather all relevant element values
and pack them into an array.

The function \texttt{out{\_}pack{\_}items} knows the kind of values
is has to gather by the flag \texttt{cc{\_}restart{\_}element}
({}``cc'' stands for {}``chunk content''). Actually there is a
further \texttt{out{\_}pack{\_}{*}} subfunction to each {}``cc''
flag and these are the ones one must look at when element io has to
be changed.

Such a functions is called for each processor in turn and has to find
all elements that have to go to this particular processor and pack
them. Packing means to copy the relevant values to the memory provided
by the caller. This first thing to do is to remember the number of
integers and doubles each element is allowed to contribute. It might
contribute less, leaving some space unoccupied, but it must not write
more that this: 

\begin{quote}
\texttt{len~=~chunk->value{\_}entry{\_}length;~}~\\
 \texttt{slen~=~chunk->size{\_}entry{\_}length; }
\end{quote}
Afterwards the function loops all elements in the current partition
and finds each one that is to be written by the specific processor.
The arguments \texttt{dst{\_}first{\_}id} and \texttt{dst{\_}num}
give the id of the first element this processor is interested in and
the total number of elements this processor has to write. Of course
we know that these \texttt{dst{\_}num} elements will have the ids
\texttt{dst{\_}first{\_}id}, \texttt{dst{\_}first{\_}id+1}
and so on. The loop starts as follows: 

\begin{quote}
\texttt{counter~=~0;~}~\\
 \texttt{for~(i=0;~i<actpdis->numele;~++i)~{\{}~}~\\
 \texttt{~~ELEMENT{*}~actele~=~actpdis->element{[}i];~}~\\
 \texttt{~~if~((actele->Id{\_}loc~>=~dst{\_}first{\_}id)~{\&}{\&}~}~\\
 \texttt{~~~~~~(actele->Id{\_}loc~<~dst{\_}first{\_}id+dst{\_}num))~{\{}~}~\\
 \texttt{~~~~double{*}~dst{\_}ptr;~}~\\
 \texttt{~~~~int{*}~size{\_}dst{\_}ptr; }
\end{quote}
Please note that we know the elements are ordered by their \texttt{Id{\_}loc},
but we don't own a consecutive list. After all, we are just one processor
in a massively parallel calculation, we own some elements but nobody
said their \texttt{Id{\_}loc} would be consecutive. However we know
the range of elements the receiving processor expects and every element
that falls in that range will be packed.

The two pointers \texttt{dst{\_}ptr} and \texttt{size{\_}dst{\_}ptr}
are used to point to the place in memory where the current element's
values are to be written. The memory is provided by the caller and
is just big enough to hold all elements of this partition that have
to go to that (the receiving) processor. In the parallel case the
sizes array is enhanced by one integer per element and each entry
must start with the element's \texttt{Id{\_}loc}: 

\begin{quote}
\texttt{{\#}ifdef~PARALLEL~}~\\
 \texttt{~~~~send{\_}size{\_}buf{[}(slen+1){*}counter]~=~actele->Id{\_}loc;~}~\\
 \texttt{~~~~size{\_}dst{\_}ptr~=~{\&}(send{\_}size{\_}buf{[}(slen+1){*}counter+1]);~}~\\
 \texttt{{\#}else~}~\\
 \texttt{~~~~size{\_}dst{\_}ptr~=~{\&}(send{\_}size{\_}buf{[}slen{*}counter]);~}~\\
 \texttt{{\#}endif~}~\\
 \texttt{~}~\\
 \texttt{~~~~dst{\_}ptr~=~{\&}(send{\_}buf{[}len{*}counter]); }
\end{quote}
The receiving processor needs this additional integer in the parallel
case to know to which element this entry belongs. However, the \texttt{Id{\_}loc}
here won't be written to the output files.

After this setup the interesting data has to be identified and copied
(for each element, we are inside the loop): 

\begin{quote}
\texttt{switch~(actele->eltyp)~{\{}~}~\\
 \texttt{~}~\\
 \texttt{~~/{*}~Do~all~elements~that~might~appear~in~such~a~chunk.~{*}/~}~\\
 \texttt{~}~\\
 \texttt{default:~}~\\
 \texttt{~~dserror(\char`\"{}element~type~{\%}d~unsupported\char`\"{},~actele->eltyp);~}~\\
 \texttt{{\}} }
\end{quote}
And finally we advance the counter that controls the placement of
the element's data in the memory buffers: 

\begin{quote}
\texttt{~~~~counter~+=~1;~}~\\
 \texttt{~~{\}}~}~\\
 \texttt{{\}} }
\end{quote}
This way all element data is collected and can be transfered to the
processor that's supposed to write it.

That's a rough sketch of what happens when writing restart data. The
big gap in the middle remains to be filled with advanced material.
See below.


\subsection{Reading}

The function \texttt{restart{\_}read{\_}bin{\_}fluiddyn} is
very similar to its write counterpart. However, there are differences.
Each field is read just once, that's why there's no strange \texttt{context}
argument here but instead we have to initialize a local context variable.
That's done by the function call: 

\begin{quote}
\texttt{init{\_}bin{\_}in{\_}field({\&}context,~sysarray{\_}typ,~sysarray,~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~actfield,~actpart,~actintra,~disnum); }
\end{quote}
This function figures out which processor reads what part of the data
and opens the discretization's data files named by the control file.
All the internal variables are set up properly. And because there
is dynamic memory involved we need to clean things up afterwards.
Thus the last line of \texttt{restart{\_}read{\_}bin{\_}fluiddyn}
is: 

\begin{quote}
\texttt{destroy{\_}bin{\_}in{\_}field({\&}context); }
\end{quote}
Between these two calls there are again the three parts already known
from writing. At first the corresponding group from the control file
is searched: 

\begin{quote}
\texttt{result{\_}info~=~in{\_}find{\_}restart{\_}group(actfield,~disnum,~step); }
\end{quote}
This will search for a group called \texttt{restart} with matching
field name, discretization number and step count. If there is no such
group the input is invalid and ccarat comes down via \texttt{dserror}.
That's the general idea of ccarat's input facilities: Stop if something
went wrong. In this call the whole control file magic is hidden.

In the second part the node arrays are read. The \texttt{sol{\_}mf}
array might not be there so we have to check first: 

\begin{quote}
\texttt{in{\_}node{\_}arrays({\&}context,~result{\_}info,~node{\_}array{\_}sol);~}~\\
 \texttt{in{\_}node{\_}arrays({\&}context,~result{\_}info,~node{\_}array{\_}sol{\_}increment);~}~\\
 \texttt{if~(map{\_}has{\_}map(result{\_}info,~\char`\"{}sol{\_}mf\char`\"{}))~{\{}~}~\\
 \texttt{~~in{\_}node{\_}arrays({\&}context,~result{\_}info,~node{\_}array{\_}sol{\_}mf);~}~\\
 \texttt{{\}} }
\end{quote}
And finally we need to read the element data: 

\begin{quote}
\texttt{in{\_}element{\_}chunk({\&}context,~result{\_}info,~\char`\"{}element{\_}data\char`\"{},~cc{\_}restart{\_}element); }
\end{quote}
This, again, needs some more attention because it's a element specific
read. This time the io module has means to figure out how large the
data to be read are (these numbers are in the control file) but the
ability to store the values back into the elements must be provided
for each kind of chunk.

The input function that matches \texttt{out{\_}pack{\_}items}
is called \texttt{in{\_}unpack{\_}items} and lives in \texttt{io{\_}packing.c}
as well. It calls the corresponding unpack function, \texttt{in{\_}unpack{\_}restart{\_}element}
in case of a restart read.

This functions is called once for each processor that reads data.
Each time one processor's data is handed in and the task consists
of distributing it to the elements. For this reason there is a loop
over the received items: 

\begin{quote}
\texttt{for~(el=0;~el<get{\_}recv{\_}numele;~++el)~{\{} }
\end{quote}
For each item the receiving array is found and the values are copied.
Of course that's again element specific: 

\begin{quote}
\texttt{switch~(actele->eltyp)~{\{}~}~\\
 \texttt{~}~\\
 \texttt{~~/{*}~Handle~reading~all~kinds~of~elements~{*}/~}~\\
 \texttt{~}~\\
 \texttt{default:~}~\\
 \texttt{~~dserror(\char`\"{}element~type~{\%}d~unsupported\char`\"{},~actele->eltyp);~}~\\
 \texttt{{\}} }
\end{quote}
A slight complication araises again from the distinction between sequential
and parallel version. The sequential version knows that all elements
are read on its one and only processor. Therefore it simply loops
all its elements. The parallel version, however, handles each source
processor in turn. So the number of elements in the loop depends on
the current source processor and has to be looked up from the context
data. The same goes for the element's position inside the local partition.
These peculiarities, however, can be hidden by the preprocessor so
that we can use the same code for both versions.


\subsection{Distributed Vectors}

The restart functions for structure algorithms need to store distributed
vectors additional to node and element arrays. However, that easy.
There are the functions \texttt{out{\_}distvec{\_}chunk} and \texttt{in{\_}distvec{\_}chunk}
that do for these vectors what \texttt{out{\_}node{\_}arrays}
and \texttt{in{\_}node{\_}arrays} do for node arrays. There's
no user supplied low level code needed.


\subsection{Restart conclusion}

The \texttt{restart{\_}write{\_}{*}} and \texttt{restart{\_}read{\_}{*}}
functions from \texttt{io{\_}restart.h} are public functions of
the io module, but at the same time these are functions any developer
might want to write. But that's easy because they build upon the io
module's {}``internal interface'', a set of functions that is meant
to be used within the io module without showing too much of the implementation
details.

To handle the elements one has to consider low level details. There
is no way to avoid that. But there's a clean cut: You can circumvent
the main machinery and just look at how data is extracted from and
put into elements.


\subsection{Final restart remark}

The nice thing about the new restart is that you don't need to move
your files even when you restart many times. The restart setup will
automatically find a new name for your files that has not been used
before. Imagine a restart call like the following: 

\begin{quote}
\texttt{./ccarat~inputfile~outputname~restart }
\end{quote}
Here the \texttt{outputname} will be changed to \texttt{outputname-1}.
If there's a file called \texttt{outputname-1.control} the name will
change to \texttt{outputname-2} and so on. However, if you start with: 

\begin{quote}
\texttt{./ccarat~inputfile~outputname-7~restart }
\end{quote}
the first try will be \texttt{outputname-8}, going on to \texttt{outputname-9}.
But please note that only the existence of the control file is tested.
That's fine as long as you don't change the data files' names. Also
note that it might be a bad idea to end your output names with a minus
sign and a number.


\section{Postprocessing output}

The general idea of restart is to copy data from memory to the disc
and be able to copy it back. No transformations whatsoever are performed.
The postprocessing output is different in this respect. Here meaningful
data is to be written. In particular data that can be interpreted
outside of ccarat's internal data structures. So it's no longer sufficient
to simply dump all the node arrays. Instead we are interested in a
few specific entries, one row for example.

It follows that there is no use to read this data back into ccarat.
(Instead we have to write filter applications that generate input
suitable for GiD and others.) So the output part of the io module
is going to be bigger that the input part.

The topmost output function for general postprocessing output is the
\texttt{out{\_}results} function that has already been used in the
example above. It is again defined in \texttt{in{\_}packing.c}.
You tell it what kinds of output you want to get. The supported output
types are the ones known from the old text output: 

\begin{itemize}
\item {} displacement
\item {} velocity
\item {} pressure
\item {} stress
\item {} contact
\item {} eigenmodes
\item {} thickness
\item {} axi{\_}loads
\end{itemize}
It's the task of this function to figure out what type of problem
is running and what kind of chunks are needed accordingly. In order
to do so it relies on the knowledge about the types of elements in
the current discretization that was gathered during initialization.
(See below.)

The function \texttt{out{\_}results} is a public one like the restart
functions and the same reasoning applies here. You might want to enhance
it, but you should be able to get by with the {}``internal interface''
functions of the io module apart from the element handling. To do
special things with elements you might have to introduce new \texttt{cc{\_}{*}}
constants and corresponding low level functions. The mechanism is
the same as explained for the restart case above.


\section{Types of Elements}

The function \texttt{init{\_}bin{\_}out{\_}fluid} has to figure
out, among other things, what kind of elements there are in this discretization.
However, the simple \texttt{ELEMENT{\_}TYP} enum is not sufficient
for the purpose of io. Additional to the element type already present
in ccarat the number of nodes and gauss points is important. So each
triple (type, {\#}nodes, {\#}gauss points) counts as different
kind of element.

In order to keep things manageable I introduced a new classification
scheme that consists of major and minor version numbers. Each possible
combination of element type, node numbers and gauss point numbers
is identified by a unique major, minor pair where the major number
coincides with the common element type and the minor number steams
from a consecutive counting of the element's variants. This way the
\texttt{ELEMENT{\_}FLAGS} type can be defined to be a two dimensional
array with fixed size. The function \texttt{out{\_}find{\_}element{\_}types}
initializes this array to zero and afterwards sets each entry corresponding
to an element type found in the discretization to one.

This arrangement might look like an implementation detail but it has
some important consequences. First of all it enables us to handle
different element types in a systematic manner. There's the global
variable \texttt{element{\_}info} (from \texttt{global{\_}element{\_}info.c})
that contains information about the different element variants known
to ccarat by major and minor number. Secondly we can use the major
and minor numbers to identify the elements in the output. That's obviously
needed because the postprocessing output needs to be interpreted without
the ccarat input file's aid. Therefore the type of each element has
to be written along with the mesh connectivity.

These numbers are written to result files and those files are going
to stay around for a while, so the meaning of these numbers must be
preserved. On the other hand are the values of these numbers implementation
details, we simply cannot guarantee that there will be no changes.
For this reason we need to distinguish internal and external versions
of these number.

It's really very simple. The ccarat code always works with the internal
numbers. The numbers written to a file are always external. This demands
that on reading we convert the external numbers we read to internal
numbers we can work with. (On writing no explicit conversion is needed.)
To facilitate this conversion there are \texttt{element{\_}variant}
groups in each discretization's main group in the control file. For
example a 2d fluid problem might contain the group: 

\begin{quote}
\texttt{element{\_}variant:~}~\\
 \texttt{~~~~type~=~\char`\"{}fluid2\char`\"{}~}~\\
 \texttt{~~~~major~=~7~}~\\
 \texttt{~~~~minor~=~0~}~\\
 \texttt{~~~~numnp~=~4~}~\\
 \texttt{~~~~nGP0~=~2 }
\end{quote}
This states that the element with the (external) major number 7 and
minor number 0 used in this discretization is a fluid2 element with
four nodes and two gauss points (in each direction).

There must be a \texttt{element{\_}variant} group for every type
of element used in a discretization. On reading the filters are able
to find the internal major and minor numbers corresponding to the
given criteria (fluid2, four nodes, two gauss points in this case).

Please note that due to the flexibility of the control file it's possible
to specify any criteria needed to distinguish an element variant.


\section{Mesh Connectivity}

As mentioned above we have to store the element connectivity along
with each discretization. The \texttt{init{\_}bin{\_}out{\_}field}
function does this automatically, utilizing the normal io facilities.
The relevant lines are close to the end of this function: 

\begin{quote}
\texttt{find{\_}mesh{\_}item{\_}length(context,~{\&}value{\_}length,~{\&}size{\_}length);~}~\\
 \texttt{out{\_}element{\_}chunk(context,~\char`\"{}mesh\char`\"{},~cc{\_}mesh,~value{\_}length,~size{\_}length); }
\end{quote}
Here \texttt{find{\_}mesh{\_}item{\_}length} figures out the
length of both types of entries, the entries in the value array as
well as the entries in the size array. Keep in mind that both data
files consist of individual chunks, each chunk beeing divided into
entries of constant length. This way the entries can be accessed by
their index.

The next line in \texttt{init{\_}bin{\_}out{\_}field} stores
the node's coordinates. To do this \texttt{genprob.ndim} double values
are needed and one integer (that's known and thus no find{\_}{*}
function is needed here): 

\begin{quote}
\texttt{out{\_}node{\_}chunk(context,~\char`\"{}coords\char`\"{},~cc{\_}coords,~value{\_}length,~size{\_}length,~0); }
\end{quote}
These function calls write the \texttt{mesh} and \texttt{coords} chunks
we've already discussed above.


\section{The communication layer}

The functional core of the io module is the communication layer that
lives in \texttt{io{\_}singlefile.c}. The communication happens
each time a chunk is read or written. It's the function \texttt{in{\_}scatter{\_}chunk}
that takes the data read by each processor and distributes it to the
processor where the corresponding node or element lives. On the other
side it's \texttt{out{\_}gather{\_}values} which collects the
data and distributes it to the processor where it is written. This
communication process happens in a chattering style.


\subsection{Chattering processors communication pattern}

This is a particular clever way to arrange your code when each pair
of processors needs to exchange a different piece of information.
That's in contrast to the famous round robin scheme which is used
when each processor has got some piece of information that needs to
be communicated to everyone. I learnt about this pattern from Dr.
Behr, but he claimed not to be the originator of it so I refrained
from using his name. (I was tempted, though.) Peter suggested the
name chattering processors.

The central idea of this scheme is to arrange the processor in a circular
list. Then each processor sends data to its right neighbour and receives
from its left. There is the \texttt{MPI{\_}Sendrecv} function that
handles sending and receiving at the same time. In the next step data
is sent to the right but one neighbour and received from the left
but one neighbour accordingly. And so on until every processor has
talked to every other.

The central point is to find in each iteration \texttt{i} the processor
the local one has to talk to: 

\begin{quote}
\texttt{dst~=~(rank~+~i~+~1)~{\%}~nprocs;~}~\\
 \texttt{src~=~(nprocs~+~rank~-~i~-~1)~{\%}~nprocs; }
\end{quote}
If we know how many numbers we have to send and receive the MPI call
itself is pretty simple: 

\begin{quote}
\texttt{err~=~MPI{\_}Sendrecv(send{\_}buf,~send{\_}count,~MPI{\_}double,~dst,~tag{\_}base+i,~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~~recv{\_}buf,~recv{\_}count,~MPI{\_}double,~src,~tag{\_}base+i,~}~\\
 \texttt{~~~~~~~~~~~~~~~~~~~context->actintra->MPI{\_}INTRA{\_}COMM,~{\&}status); }
\end{quote}
To figure out these counts some more communication is needed. But
luckily these counts are the same for all chunks of one type (in one
discretization). Thus we can find them in the setup phase (in \texttt{init{\_}bin{\_}in{\_}field}
and \texttt{init{\_}bin{\_}out{\_}field}) and use them afterwards.

There are two variants to this communication pattern that differ in
whether each processor has to talk to itself. In this case that's
needed, thus the loop starts like this: 

\begin{quote}
\texttt{for~(i=0;~i<nprocs;~++i)~{\{} }
\end{quote}
and in the last iteration \texttt{src == dst == rank}. But this case
is handled by \texttt{MPI{\_}Sendrecv} equally well (without communication).
No special attention is needed.


\section{Module data structures}

The io module is split into an input and an output part. In each part
there is a data structure hierarchy consisting of the main structure
(\texttt{BIN{\_}IN{\_}MAIN}, \texttt{BIN{\_}OUT{\_}MAIN}),
the per discretization structure (\texttt{BIN{\_}IN{\_}FIELD},
\texttt{BIN{\_}OUT{\_}FIELD}) and the per chunk structure (\texttt{BIN{\_}IN{\_}CHUNK},
\texttt{BIN{\_}OUT{\_}CHUNK}). There is exactly one object of
the main structure.


