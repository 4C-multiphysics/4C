/*!----------------------------------------------------------------------
\file linalg_utils.H
\brief A collection of helper methods for namespace LINALG

<pre>
-------------------------------------------------------------------------
                 BACI finite element library subsystem
            Copyright (2008) Technical University of Munich

Under terms of contract T004.008.000 there is a non-exclusive license for use
of this work by or on behalf of Rolls-Royce Ltd & Co KG, Germany.

This library is proprietary software. It must not be published, distributed,
copied or altered in any form or any media without written permission
of the copyright holder. It may be used under terms and conditions of the
above mentioned license by or on behalf of Rolls-Royce Ltd & Co KG, Germany.

This library may solemnly used in conjunction with the BACI contact library
for purposes described in the above mentioned contract.

This library contains and makes use of software copyrighted by Sandia Corporation
and distributed under LGPL licence. Licensing does not apply to this or any
other third party software used here.

Questions? Contact Dr. Michael W. Gee (gee@lnm.mw.tum.de)
                   or
                   Prof. Dr. Wolfgang A. Wall (wall@lnm.mw.tum.de)

http://www.lnm.mw.tum.de

-------------------------------------------------------------------------
</pre>
<pre>
Maintainer: Michael Gee
            gee@lnm.mw.tum.de
            http://www.lnm.mw.tum.de
            089 - 289-15239
</pre>

*----------------------------------------------------------------------*/
#ifdef CCADISCRET
#ifndef LINALG_UTILS_H
#define LINALG_UTILS_H


#include "Epetra_Comm.h"
#include "Epetra_Map.h"
#include "Epetra_CrsGraph.h"
#include "Epetra_CrsMatrix.h"
#include "Epetra_Vector.h"
#include "Epetra_Export.h"
#include "Epetra_Import.h"
#include "Epetra_SerialDenseMatrix.h"
#include "Epetra_SerialDenseVector.h"
#include "Epetra_LAPACK.h"
#include "Teuchos_RefCountPtr.hpp"
#include "Ifpack_AdditiveSchwarz.h"
#include "drt_exporter.H"
#include "linalg_blocksparsematrix.H"

using namespace std;
using namespace Teuchos;

namespace IO
{
class DiscretizationWriter;
}

namespace LINALG
{
  // forward declaration
  class Solver;

  /*!
  \brief Create a new Epetra_CrsMatrix and return RefcountPtr to it

  \param rowmap (in): row map of matrix
  \param npr (in): estimated number of entries per row.
                   (need not be exact, better should be too big rather then too small)
  */
  RefCountPtr<Epetra_CrsMatrix> CreateMatrix(const Epetra_Map& rowmap, const int npr);

  /*!
  \brief Create a new Epetra_Vector and return RefcountPtr to it

  \param rowmap (in): row map of vector
  \param init (in): initializa vector to zero upon construction
  */
  RefCountPtr<Epetra_Vector> CreateVector(const Epetra_Map& rowmap, const bool init = true);

  /*!
  \brief Export a vector to a different map

  Values of source are copied to target where maps don't have to match.
  Prerequisite: Either the map of source OR the map of target has to be unique
                (will be tested)
  \warning When source is overlapping (and therefore target is unique), values
           in the overlapping region are inserted into the target on a first come
           first serve basis, meaning they should be equal in the source to
           be deterministic
  \param source (in) : source vector values are taken from
  \param target (out): target vector values will be inserted in
  */
  void Export(const Epetra_MultiVector& source, Epetra_MultiVector& target);

  /*!
  \brief Make Epetra_SerialDenseMatrix symmetric by averaging upper and lower traingular indices

  \param A (in/out): Matrix to be symmetrised
  */
  void SymmetriseMatrix(Epetra_SerialDenseMatrix& A);

  /*!
  \brief Compute all eigenvalues of a real symmetric matrix A

  \param A (in):        Matrix to be analysed
  \param L (out):       Vector of eigenvalues in ascending order
  \param postproc (in): flag indicating whether we are using this
                        routine for postprocessing only (in that
                        case dserror is replaced with a warning)
  */
  void SymmetricEigenValues(Epetra_SerialDenseMatrix& A,
                            Epetra_SerialDenseVector& L,
                            const bool postproc=false);

  /*!
  \brief Compute all eigenvalues and eigenvectors of a real symmetric matrix A

  \param A (in/out):    in: Matrix to be analysed, out: eigenvectors
                        (i.e. original matrix is destroyed!!!)
  \param L (out):       Vector of eigenvalues in ascending order
  \param postproc (in): flag indicating whether we are using this
                        routine for postprocessing only (in that
                        case dserror is replaced with a warning)
  */
  void SymmetricEigenProblem(Epetra_SerialDenseMatrix& A,
                             Epetra_SerialDenseVector& L,
                             const bool postproc=false);

  void SymmetricEigenProblem(IO::DiscretizationWriter& output,
                             LINALG::Solver& solver,
                             RCP<LINALG::SparseMatrix> A,
                             RCP<LINALG::SparseMatrix> M,
                             Epetra_Vector& invtoggle,
                             int nev,
                             int step=0,
                             double time=0.0);
  /*!
  \brief Compute all eigenvalues and, optionally, eigenvectors
   of a real symmetric matrix A

  \param A (in/out):    Matrix to be analysed, if eigv=true stores eigenvectors
  \param L (in/out):    Vector of eigenvalues in ascending order
  \param eigv (in):     flag to evaluate also eigenvectors ('N'=no, 'V'=yes)
  \param postproc (in): flag indicating whether we are using this
                        routine for postprocessing only (in that
                        case dserror is replaced with a warning)
  */
  void SymmetricEigen(Epetra_SerialDenseMatrix& A,
                      Epetra_SerialDenseVector& L,
                      const char eigv,
                      const bool postproc=false);

  /*!
    \brief Compute all eigenvalues and eigenvectors of a real symmetric matrix A

            A = V * S * VT

    \param A (in):        M-by-M matrix to be decomposed
    \param S (out):       M-by-N matrix which is zero except for its diagonal entries holding the eigenvalues
    \param V (out):       M-by-M orthonormal matrix of eigenvectors
  */
  template <unsigned int dim>
  void SYEV(LINALG::Matrix<dim,dim>& A,
            LINALG::Matrix<dim,dim>& S,
            LINALG::Matrix<dim,dim>& V)
  {

    const char jobz = 'V';  // Compute eigenvalues and eigenvectors.
    const char uplo = 'U';  // Upper triangle of A is stored;
    const int N = dim; // The order of the matrix A.  N >= 0.
    Matrix<dim,dim> tmp(A.A(),false);  // copy, because content of matrix ist destroyed
    const int lda = dim;  // The leading dimension of the array A.  LDA >=max(1,N).
    std::vector<double> w(dim);
    const int lwork = 2*dim*dim+6*dim+1;
    std::vector<double> work(lwork);
    const int liwork = 3+5*dim;
    std::vector<int> iwork(liwork);
    int info;

    Epetra_LAPACK lapack;
    lapack.SYEVD(jobz,uplo,N,tmp.A(),lda,&(w[0]),&(work[0]),lwork,&(iwork[0]),liwork,&info);

    if (info) dserror("Lapack's SYEVD returned %d",info);

    // return eigenvectors
    V.Update(tmp);

    // return eigenvalues
    S.Clear();
    for (unsigned int i=0; i<dim; ++i) S(i,i) = w[i];

    return;
  }

  /*!
  \brief Compute singular value decomposition (SVD) of a real M-by-N matrix A
                        A = U * SIGMA * transpose(V)

  \param A (in/out):    Matrix to be decomposed
  \param U (in/out):    M-by-M orthogonal matrix
  \param SIGMA (in/out):M-by-N matrix which is zero except for its min(m,n) diagonal elements
  \param Vt (in/out):   V is a N-by-N orthogonal matrix, actually returned is V^T
  */
  void SVD( const Epetra_SerialDenseMatrix& A,
            LINALG::SerialDenseMatrix& U,
            LINALG::SerialDenseMatrix& SIGMA,
            LINALG::SerialDenseMatrix& Vt);


  /*!
    \brief Singular value decomposition (SVD) of a real M-by-N matrix in fixed
    size format

            A = Q * S * VT

    \param A (in):        M-by-N matrix to be decomposed
    \param Q (out):       M-by-M orthogonal matrix
    \param S (out):       M-by-N matrix which is zero except for its min(m,n) diagonal elements
    \param VT (out):      N-by-N orthogonal matrix (transpose of V)
  */
  template <unsigned int rows, unsigned int cols>
  void SVD(LINALG::Matrix<rows,cols>& A,
           LINALG::Matrix<rows,rows>& Q,
           LINALG::Matrix<rows,cols>& S,
           LINALG::Matrix<cols,cols>& VT)
  {
    Matrix<rows,cols> tmp(A.A(),false);  // copy, because content of matrix ist destroyed
    Epetra_LAPACK lapack;
    const char jobu = 'A';  // compute and return all M columns of U
    const char jobvt = 'A'; // compute and return all N rows of V^T
    std::vector<double> s(std::min(rows,cols));
    int info;
    int lwork = max(3*std::min(rows,cols)+std::max(rows,cols),5*std::min(rows,cols));
    std::vector<double> work(lwork);

    lapack.GESVD(jobu,jobvt,rows,cols,tmp.A(),tmp.M(),&s[0],
                 Q.A(),Q.M(),VT.A(),VT.M(),&work[0],&lwork,&info);

    if (info) dserror("Lapack's dgesvd returned %d",info);

    for (unsigned int i = 0; i < std::min(rows,cols); ++i) {
      for (unsigned int j = 0; j < std::min(rows,cols); ++j) {
        S(i,j) = (i==j) * s[i];   // 0 for off-diagonal, otherwise s
      }
    }
    return;
  }


  /*!
  \brief Explicit inverse and determinant of nonsymmetric 3x3 matrix

  \param A (in/out): Matrix to be inverted
  \param dim (in) :  Dimension of matrix
  */
  double NonsymInverse3x3(Epetra_SerialDenseMatrix& A);

  /*!
  \brief Determinant of a nonsymmetric matrix using SVD

  \return the determinant of the matrix A if nonzero
  */
  double DeterminantSVD(const Epetra_SerialDenseMatrix& A);


  /*!
  \brief Determinant of a nonsymmetric matrix using LU

  \return the determinant of the matrix A
  */
  double DeterminantLU(const Epetra_SerialDenseMatrix& A);

  /*!
  \brief Invert a symmetric dim*dim square matrix

  \param A (in/out): Matrix to be inverted
  \param dim (in) :  Dimension of matrix
  */
  void SymmetricInverse(Epetra_SerialDenseMatrix& A, const int dim);

  /*!
  \brief Invert a nonsymmetric dim*dim square matrix

  \param A (in/out): Matrix to be inverted
  \param dim (in) :  Dimension of matrix
  */
  void NonSymmetricInverse(Epetra_SerialDenseMatrix& A, const int dim);

  /*!
  \brief Assemble a Epetra_SerialDenseMatrix into a Epetra_CrsMatrix

  This is an individual call.
  Will only assemble locally and will never do any commmunication.
  All values that can not be assembled locally will be ignored.
  Will use the communicator and rowmap from matrix A to determine ownerships.
  Local matrix Aele may be \b square or \b rectangular.

	This version of 'Assemble' does not work for a matrix A that is already
	Filled()! If matrix A is not Filled(), it will be enlarged as required.

	\note The user must provide an \b additional input vector 'lmcol'
				containing the column gids for assembly seperately!

  \param A (out)         : Sparse matrix to be assembled on
  \param Aele (in)       : dense matrix to be assembled
  \param lmrow (in)      : vector with row gids
  \param lmrowowner (in) : vector with owner procs of row gids
  \param lmcol (in)      : vector with column gids
  */
  void Assemble(Epetra_CrsMatrix& A, const Epetra_SerialDenseMatrix& Aele,
                const vector<int>& lmrow, const vector<int>& lmrowowner,
                const vector<int>& lmcol);

  /*!
  \brief Assemble a Epetra_SerialDenseVector into a Epetra_Vector

  This is an individual call.
  Will only assemble locally and will never do any commmunication.
  All values that can not be assembled locally will be ignored.
  Will use the communicator from vector V to determine ownerships.

  \param V (out)   : Vector to be assembled on
  \param Vele (in) : dense vector to be assembled
  \param lm (in) : vector with gids
  \param lmowner (in) : vector with owner procs of gids
  */
  void Assemble(Epetra_Vector& V, const Epetra_SerialDenseVector& Vele,
                const vector<int>& lm, const vector<int>& lmowner);

  /*!
  \brief Assemble a Epetra_SerialDenseVector into a Epetra_MultiVector

  This is an individual call.
  Will only assemble locally and will never do any commmunication.
  All values that can not be assembled locally will be ignored.
  Will use the communicator from vector V to determine ownerships.

  \param V (out)   : Vector to be assembled on
  \param n (in)   : column index of MultiVector to be assembled on
  \param Vele (in) : dense vector to be assembled
  \param lm (in) : vector with gids
  \param lmowner (in) : vector with owner procs of gids
  */
  void Assemble(Epetra_MultiVector& V, const int n, const Epetra_SerialDenseVector& Vele,
                const vector<int>& lm, const vector<int>& lmowner);


  /*!
  \brief Call FillComplete on a Epetra_CrsMatrix (for square matrices only!)
  */
  void Complete(Epetra_CrsMatrix& A);

  /*!
  \brief Call FillComplete on a Epetra_CrsMatrix (for square matrices only!)

  This is the RCP wrapper of the above method.
  */
  inline void Complete(RCP<Epetra_CrsMatrix> A)
  { LINALG::Complete(*A); return; }

  /*!
  \brief Call FillComplete on a Epetra_CrsMatrix (for rectangular and square matrices)
  */
  void Complete(Epetra_CrsMatrix& A, const Epetra_Map& domainmap, const Epetra_Map& rangemap);

  /*!
  \brief Call FillComplete on a Epetra_CrsMatrix (for rectangular and square matrices)

  This is the RCP wrapper of the above method.
  */
  inline void Complete(RCP<Epetra_CrsMatrix> A, const Epetra_Map& domainmap, const Epetra_Map& rangemap)
  { LINALG::Complete(*A,domainmap,rangemap); return; }

  /*!
  \brief Add a (transposed) Epetra_CrsMatrix to another: B = B*scalarB + A(^T)*scalarA

  Add one matrix to another. the matrix B to be added to must not be
  completed. Sparsity patterns of A and B need not match and A and B can be
  nonsymmetric in value and pattern.
  Row map of A has to be a processor-local subset of the row map of B.


  Note that this is a true parallel add, even in the transposed case!

  \param A          (in)     : Matrix to add to B (must have Filled()==true)
  \param transposeA (in)     : flag indicating whether transposed of A should be used
  \param scalarA    (in)     : scaling factor for A
  \param B          (in/out) : Matrix to be added to (must have Filled()==false)
  \param scalarB    (in)     : scaling factor for B
  */
  void Add(const Epetra_CrsMatrix& A, const bool transposeA, const double scalarA,
           Epetra_CrsMatrix& B, const double scalarB);

  /*!
  \brief Add a (transposed) Epetra_CrsMatrix to another: B = B*scalarB + A(^T)*scalarA

  Add one matrix to another. the matrix B to be added to must not be
  completed. Sparsity patterns of A and B need not match and A and B can be
  nonsymmetric in value and pattern.
  Row map of A has to be a processor-local subset of the row map of B.


  Note that this is a true parallel add, even in the transposed case!
  This is the RCP wrapper of the above method.

  \param A          (in)     : Matrix to add to B (must have Filled()==true)
  \param transposeA (in)     : flag indicating whether transposed of A should be used
  \param scalarA    (in)     : scaling factor for A
  \param B          (in/out) : Matrix to be added to (must have Filled()==false)
  \param scalarB    (in)     : scaling factor for B
  */
  inline void Add(const RCP<Epetra_CrsMatrix> A,
                  const bool transposeA,
                  const double scalarA,
                  RCP<Epetra_CrsMatrix> B,
                  const double scalarB)
  { LINALG::Add(*A,transposeA,scalarA,*B,scalarB); return; }

  /*!
  \brief Compute transposed matrix of an Epetra_CrsMatrix explicitly

  Returns RCP to the transposed matrix of the input matrix A.

  \param A          (in)     : Matrix to transpose (must have Filled()==true)

  */
  RCP<Epetra_CrsMatrix> Transpose(const Epetra_CrsMatrix& A);

  /*!
  \brief Compute transposed matrix of an Epetra_CrsMatrix explicitly

  Returns RCP to the transposed matrix of the input matrix A.
  This is the RCP wrapper of the above method.

  \param A          (in)     : Matrix to transpose (must have Filled()==true)

  */
  inline RCP<Epetra_CrsMatrix> Transpose(const RCP<Epetra_CrsMatrix> A)
  { return LINALG::Transpose(*A); }

  /*!
  \brief Multiply a (transposed) Epetra_CrsMatrix with another (transposed): C = A(^T)*B(^T)

  Multiply one matrix with another. Both matrices must be completed. Sparsity
  Respective Range, Row and Domain maps of A(^T) and B(^T) have to match.

  Note that this is a true parallel multiplication, even in the transposed case!

  \param A          (in)     : Matrix to multiply with B (must have Filled()==true)
  \param transA     (in)     : flag indicating whether transposed of A should be used
  \param B          (in)     : Matrix to multiply with A (must have Filled()==true)
  \param transB     (in)     : flag indicating whether transposed of B should be used
  \param complete   (in)     : flag indicating whether FillComplete should be called on C upon exit,
                               (defaults to true)
  \return Matrix product A(^T)*B(^T)
  */
  RCP<Epetra_CrsMatrix> Multiply(const Epetra_CrsMatrix& A, bool transA,
                                 const Epetra_CrsMatrix& B, bool transB,
                                 bool complete = true);

  /*!
  \brief Multiply a (transposed) Epetra_CrsMatrix with another (transposed): C = A(^T)*B(^T)

  Multiply one matrix with another. Both matrices must be completed. Sparsity
  Respective Range, Row and Domain maps of A(^T) and B(^T) have to match.

  Note that this is a true parallel multiplication, even in the transposed case!
  This is the RCP wrapper of the above method.

  \param A          (in)     : Matrix to multiply with B (must have Filled()==true)
  \param transA     (in)     : flag indicating whether transposed of A should be used
  \param B          (in)     : Matrix to multiply with A (must have Filled()==true)
  \param transB     (in)     : flag indicating whether transposed of B should be used
  \param complete   (in)     : flag indicating whether FillComplete should be called on C upon exit,
                               (defaults to true)
  \return Matrix product A(^T)*B(^T)
  */
  inline RCP<Epetra_CrsMatrix> Multiply(const RCP<Epetra_CrsMatrix>& A, bool transA,
                                        const RCP<Epetra_CrsMatrix>& B, bool transB,
                                        bool complete = true)
  { return Multiply(*A,transA,*B,transB,complete); }

  /*!
  \brief Triple matrix product: D = A(^T)*B(^T)*C(^T)

  Multiply one matrix with another. All input matrices must be completed. Sparsity
  Respective Range, Row and Domain maps of A(^T) and B(^T) C(^T) have to match.

  Note that this is a true parallel multiplication, even in the transposed case!

  \param A          (in)     : Matrix to multiply with B (must have Filled()==true)
  \param transA     (in)     : flag indicating whether transposed of A should be used
  \param B          (in)     : Matrix to multiply with C (must have Filled()==true)
  \param transB     (in)     : flag indicating whether transposed of B should be used
  \param C          (in)     : Matrix C (must have Filled()==true)
  \param transC     (in)     : flag indicating whether transposed of C should be used
  \param complete   (in)     : flag indicating whether FillComplete should be called on C upon exit,
                               (defaults to true)
  \return Matrix product A(^T)*B(^T)*C(^T)
  */
  RCP<Epetra_CrsMatrix> Multiply(const Epetra_CrsMatrix& A, bool transA,
                                 const Epetra_CrsMatrix& B, bool transB,
                                 const Epetra_CrsMatrix& C, bool transC,
                                 bool complete = true);

  /*!
  \brief Triple matrix product: D = A(^T)*B(^T)*C(^T)

  Multiply one matrix with another. All input matrices must be completed. Sparsity
  Respective Range, Row and Domain maps of A(^T) and B(^T) C(^T) have to match.

  Note that this is a true parallel multiplication, even in the transposed case!
  This is the RCP wrapper of the above method.

  \param A          (in)     : Matrix to multiply with B (must have Filled()==true)
  \param transA     (in)     : flag indicating whether transposed of A should be used
  \param B          (in)     : Matrix to multiply with C (must have Filled()==true)
  \param transB     (in)     : flag indicating whether transposed of B should be used
  \param C          (in)     : Matrix C (must have Filled()==true)
  \param transC     (in)     : flag indicating whether transposed of C should be used
  \param complete   (in)     : flag indicating whether FillComplete should be called on C upon exit,
                               (defaults to true)
  \return Matrix product A(^T)*B(^T)*C(^T)
  */
  inline RCP<Epetra_CrsMatrix> Multiply(const RCP<Epetra_CrsMatrix>& A, bool transA,
                                        const RCP<Epetra_CrsMatrix>& B, bool transB,
                                        const RCP<Epetra_CrsMatrix>& C, bool transC,
                                        bool complete = true)
  { return Multiply(*A,transA,*B,transB,*C,transC,complete); }

  /*!
  \brief Apply dirichlet boundary condition to a linear system of equations

  Modifies a system of equations such that dirichlet boundary conditions are enforced.
  Prescribed dirichlet BC values are supplied in dbcval and dbctoggle, where
  a prescribed value is dbcval[i] and dbctoggle[i] = 1.0. No BC is enforced in
  all places where dbctoggle[i] != 1.0.<br>
  Let us denote the \f$ A_{2 \times 2} \f$ blocks of \f$A\f$ by
  \f$A_{ff}, A_{fD}, A_{Df}, A_{DD}\f$, where \f$f\f$ stands for 'free' and
  \f$D\f$ stands for 'Dirichlet BC'. Then, after a call to this method<br>

  \f$ A_{ff} = A_{ff}, \f$<br>
  \f$ A_{fD} = A_{fD}, \f$<br>
  \f$ A_{Df} = 0_{Df}, \f$<br>
  \f$ A_{DD} = I_{DD}, \f$<br>
  \f$ x_{D} = dbcval_{D}, \f$<br>
  \f$ b_{D} = dbcval_{D} \f$<br>

  and

  \f$ A_{ff} x_f + A_{fD} x_D = b_f \f$<br>
  \f$ 0 x_f + I_{DD} x_D = x_D \f$.<br>

  \note The matrix is then nonsymmetric. When using iterative methods on this
        linear system of equations that depend on the symmetry of the matrix (such as e.g. CG),
        the initial guess supplied to the solver has to be exact at the
        Dirichlet BCs. This should be easy, as the values at the Dirichlet BCs
        are known.

  \note The mask of matrix \f$A\f$ is not modified. That is the
        entries in \f$A_{Df}\f$ and \f$A_{DD}\f$ are set to zero, not
        removed. This way the matrix can be reused in the next step.

  \param A         (in/out) : Matrix of Ax=b
  \param x         (in/out) : initial guess vector x of Ax=b
  \param b         (in/out) : rhs vector b of Ax=b
  \param dbcval    (in)     : vector holding prescribed dirichlet values
  \param dbctoggle (in)     : vector holding 1.0 where dirichlet should be applied
                              and 0.0 everywhere else
  */
  void ApplyDirichlettoSystem(RCP<SparseOperator>      A,
                              RCP<Epetra_Vector>&      x,
                              RCP<Epetra_Vector>&      b,
                              const RCP<Epetra_Vector> dbcval,
                              const RCP<Epetra_Vector> dbctoggle);

  /*!
  \brief Apply dirichlet boundary condition to a linear system of equations

  This is a flexible routine. The vectors x and dbcval might have different
  maps. The map does not need to contain all Dirichlet dofs.

  The purpose is to set Dirichlet values at a subset of all Dirichlet
  boundaries.

  \param A (in/out)         : Matrix of Ax=b
  \param x (in/out)         : vector x of Ax=b
  \param b (in/out)         : vector b of Ax=b
  \param dbcval (in)        : vector holding values that are supposed to be prescribed
  \param dbctoggle (in)     : unique map of all dofs that should be constrained

  \pre The map dbctoggle must be subset of the maps of the vectors.
  */
  void ApplyDirichlettoSystem(RCP<LINALG::SparseOperator> A,
                              RCP<Epetra_Vector>&         x,
                              RCP<Epetra_Vector>&         b,
                              const RCP<Epetra_Vector>&   dbcval,
                              const Epetra_Map&           dbcmap);

  /*!
  \brief Apply dirichlet boundary condition to a linear system of equations

  This is a flexible routine. The vectors x and dbcval might have different
  maps. The map does not need to contain all Dirichlet dofs.

  The purpose is to set Dirichlet values at a subset of all Dirichlet
  boundaries.

  Special in this routine is the ability to insert rows of general rotation 
  matrices (stored in #trafo) rather than simply put ones and zeros 
  at the rows associated Dirichlet DOFs.

  \param A (in/out)         : Matrix of Ax=b
  \param x (in/out)         : vector x of Ax=b
  \param b (in/out)         : vector b of Ax=b
  \param trafo (in)         : global matrix holding rotation matrices to convert
                              from global to local co-ordinate systems
  \param dbcval (in)        : vector holding values that are supposed to be prescribed
  \param dbctoggle (in)     : unique map of all dofs that should be constrained

  \pre The map dbctoggle must be subset of the maps of the vectors.
  */
  void ApplyDirichlettoSystem(RCP<LINALG::SparseMatrix>       A,
                              RCP<Epetra_Vector>&             x,
                              RCP<Epetra_Vector>&             b,
                              RCP<const LINALG::SparseMatrix> trafo,
                              const RCP<Epetra_Vector>&       dbcval,
                              const Epetra_Map&               dbcmap);

  /*!
  \brief Apply dirichlet boundary condition to a linear system of equations


  \param x (in/out)         : vector x of Ax=b
  \param b (in/out)         : vector b of Ax=b
  \param dbcval (in)        : vector holding values that are supposed to be prescribed
  \param dbctoggle (in)     : vector holding 1.0 where dirichlet should be applied
                              and 0.0 everywhere else
  */
  void ApplyDirichlettoSystem(RCP<Epetra_Vector>&      x,
                              RCP<Epetra_Vector>&      b,
                              const RCP<Epetra_Vector> dbcval,
                              const RCP<Epetra_Vector> dbctoggle);


  /*!
  \brief Apply dirichlet boundary condition to a linear system of equations

  This is a flexible routine. The vectors x and dbcval might have different
  maps. The map does not need to contain all Dirichlet dofs.

  The purpose is to set Dirichlet values at a subset of all Dirichlet
  boundaries.

  \param x (in/out)         : vector x of Ax=b
  \param b (in/out)         : vector b of Ax=b
  \param dbcval (in)        : vector holding values that are supposed to be prescribed
  \param dbctoggle (in)     : unique map of all dofs that should be constrained

  \pre The map dbctoggle must be subset of the maps of the vectors.
  */
  void ApplyDirichlettoSystem(RCP<Epetra_Vector>&      x,
                              RCP<Epetra_Vector>&      b,
                              RCP<const Epetra_Vector> dbcval,
                              const Epetra_Map&        dbcmap);


  /*!
  \brief Convert a Dirichlet toggle vector in a Dirichlet map

  The purpose of the routine is a smooth transition from Dirichlet toggle vectors
  to Dirichlet condition maps. Eventually, this method should be removed.

  A Dirichlet toogle vector is a real vector which holds a 1.0 at DOF subjected
  to Dirichlet boundary conditions and a 0.0 at every remaining/free DOF.

  \param dbctoggle (in)     : the Dirichlet toggle vector
  \return MapExtractor object which stores the Dirichlet condition and remaining (other) DOF map

  \author bborn
  \date 10/08
  */
  Teuchos::RCP<LINALG::MapExtractor> ConvertDirichletToggleVectorToMaps(
    const Teuchos::RCP<const Epetra_Vector>& dbctoggle);

  /*!
  \brief split a matrix into a 2x2 block system where the rowmap of one of the blocks is given

  Splits a given matrix into a 2x2 block system where the rowmap of one of the blocks is given
  on input. Blocks A11 and A22 are assumed to be square.
  All values on entry have to be Teuchos::null except the given rowmap and matrix A.
  Note that either A11rowmap or A22rowmap or both have to be nonzero. In case
  both rowmaps are supplied they have to be an exact and nonoverlapping split of A->RowMap().
  Matrix blocks are FillComplete() on exit.

  \param A         : Matrix A on input
  \param A11rowmap : rowmap of A11 or null
  \param A22rowmap : rowmap of A22 or null
  \param A11       : on exit matrix block A11
  \param A12       : on exit matrix block A12
  \param A21       : on exit matrix block A21
  \param A22       : on exit matrix block A22
  */
  bool SplitMatrix2x2(RCP<Epetra_CrsMatrix> A,
                      RCP<Epetra_Map>& A11rowmap,
                      RCP<Epetra_Map>& A22rowmap,
                      RCP<Epetra_CrsMatrix>& A11,
                      RCP<Epetra_CrsMatrix>& A12,
                      RCP<Epetra_CrsMatrix>& A21,
                      RCP<Epetra_CrsMatrix>& A22);

  /*!
  \brief split a matrix into a 2x2 block system

  Splits a given matrix into a 2x2 block system. All values on entry have to be
  Teuchos::null except the given rowmap(s) / domainmap(s) and matrix A.
  Note that either A11rowmap or A22rowmap or both have to be nonzero!
  Note that either A11domainmap or A22domainmap or both have to be nonzero!
  In case both rowmaps / domainmaps are supplied they have to be an exact and
  nonoverlapping split of A->RowMap() / A->DomainMap().
  Matrix blocks are FillComplete() on exit.

  \param A            : Matrix A on input
  \param A11rowmap    : rowmap of A11 or null
  \param A22rowmap    : rowmap of A22 or null
  \param A11domainmap : domainmap of A11 or null
  \param A22domainmap : domainmap of A22 or null
  \param A11          : on exit matrix block A11
  \param A12          : on exit matrix block A12
  \param A21          : on exit matrix block A21
  \param A22          : on exit matrix block A22
  */
  bool SplitMatrix2x2(RCP<LINALG::SparseMatrix> A,
                      RCP<Epetra_Map>& A11rowmap,
                      RCP<Epetra_Map>& A22rowmap,
                      RCP<Epetra_Map>& A11domainmap,
                      RCP<Epetra_Map>& A22domainmap,
                      RCP<LINALG::SparseMatrix>& A11,
                      RCP<LINALG::SparseMatrix>& A12,
                      RCP<LINALG::SparseMatrix>& A21,
                      RCP<LINALG::SparseMatrix>& A22);

  /*!
  \brief split a rowmap of matrix A

  splits A->RowMap() into 2 maps and returns them, where one of the rowmaps has
  to be given on input

  \param Amap      : Map to split on input
  \param Agiven    : on entry submap that is given and part of Amap
  \return the remainder map of Amap that is not overlapping with Agiven
  */
  Teuchos::RCP<Epetra_Map> SplitMap(const Epetra_Map& Amap,
                                    const Epetra_Map& Agiven);

  /*!
  \brief merges two given Epetra_Maps

  merges input map1 and input map2, both of which have to be unique,
  but may be overlapping, to a new map and returns RCP to it.

  \param map1         : one map to be merged
  \param map2         : the other map to be merged
  \param allowoverlap : when set to false, an error is thrown if the result
                        map is overlapping (default = true, overlap allowed)
  \return the (sorted) merged map of input maps map1 and map2
  */
  RCP<Epetra_Map> MergeMap(const Epetra_Map& map1,
                           const Epetra_Map& map2,
                           bool overlap = true);

  /*!
  \brief merges two given Epetra_Maps

  merges input map1 and input map2 (given as RCP), both of which
  have to be unique, but may be overlapping, to a new map and returns
  RCP to it. The case that one or both input RCPs are null is
  detected and handled appropriately.

  \param map1         : one map to be merged
  \param map2         : the other map to be merged
  \param allowoverlap : when set to false, an error is thrown if the result
                        map is overlapping (default = true, overlap allowed)
  \return the (sorted) merged map of input maps map1 and map2
  */
  RCP<Epetra_Map> MergeMap(const RCP<Epetra_Map>& map1,
                           const RCP<Epetra_Map>& map2,
                           bool overlap = true);

  /*!
  \brief split a vector into 2 non-overlapping pieces (RCP version)
  */
  bool SplitVector(const Epetra_Vector& x,
                   const Epetra_Map& x1map,
                   RCP<Epetra_Vector>&   x1,
                   const Epetra_Map& x2map,
                   RCP<Epetra_Vector>&   x2);

  /*!
  \brief Print sparsity pattern of a matrix to postscript file

  creates a file Epetra::Matrix.ps in current directory where the exact
  name of file depends on the exact type of class.

  \note works in parallel and serial!
  */
  void PrintSparsityToPostscript(const Epetra_RowMatrix& A);

  /*!
  \brief Gather information of type vector<T> on a subset of processors

  This template gathers information provided in sdata on a subset of
  processors tprocs, where the length of the array tprocs is ntargetprocs.
  The redistributed data is returned in rdata which has appropiate size
  on output (size of rdata on input is arbitrary). ntargetprocs can be
  one to reduce data to one proc, it also can be equal to the total number
  of processors to make sdata redundant on all procs.

  \note Functionality of this method is equal to that of Epetra_Comm::GatherAll
        except for that the Epetra version demands the data to be of constant
        size over all procs which this method does not require!

  \param sdata (in) : Information to be gathered on tprocs.
                      Length of sdata can be different on every proc.
  \param rdata (out): Information from sdata gathered on a subset of procs.
                      size of rdata on input is arbitrary, it is exact on output.
  \param ntargetprocs (in): length of tprocs
  \param tprocs (in): vector of procs ids the information in sdata shall be
                      gathered on.
  \param comm (in):   communicator to be used.


  */
  template<typename T> void Gather(vector<T>&         sdata,
                                   vector<T>&         rdata,
                                   const int          ntargetprocs,
                                   const int*         tprocs,
                                   const Epetra_Comm& comm)
  {
    const int myrank  = comm.MyPID();
    const int numproc = comm.NumProc();
    if (numproc==1)
    {
      rdata = sdata;
      return; // nothing to do in serial
    }
    // build a map of data
    map<int,vector<T> > datamap;
    datamap[myrank] = sdata;
    // build a source map
    Epetra_Map source(numproc,1,&myrank,0,comm);
    // build a target map which is redundant on all target procs and zero everywhere else
    bool iamtarget = false;
    for (int i=0; i<ntargetprocs; ++i)
      if (tprocs[i]==myrank)
      {
        iamtarget = true;
        break;
      }
    vector<int> targetvec(0);
    if (iamtarget)
    {
      targetvec.resize(numproc);
      for (int i=0; i<numproc; ++i) targetvec[i] = i;
    }
    const int tnummyelements = (int)targetvec.size();
    Epetra_Map target(-1,tnummyelements,&targetvec[0],0,comm);
    // build an exporter and export data
    DRT::Exporter exporter(source,target,comm);
    exporter.Export(datamap);
    // put data from map in rdata
    rdata.clear();
    int count=0;
    map<int,vector<int> >::iterator curr;
    for (curr=datamap.begin(); curr != datamap.end(); ++curr)
    {
      vector<T>& current = curr->second;
      const int size = (int)current.size();
      rdata.resize((int)rdata.size()+size);
      for (int i=0; i<size; ++i)
      rdata[count+i] = current[i];
      count += size;
    }
    return;
  }

  /// Create an allreduced vector of gids from the given Epetra_Map
  /*!
    We have nodes and elements with unique but otherwise arbitrary
    global ids. On rare occations, however, we need to allreduce a
    particular map to one or more processors. This is a building block
    for such occations. We allreduce the gids of the given Epetra_Map
    into a vector ordered by processor number.

    It is assumed that the given map does not overlap!

    \note You are not supposed to use redundant vectors in normal
    situations. If you happen to need this method you are probably
    about to do something illegal.

    \param rredundant (o) redundant vector of global ids
    \param emap (i) distributed Epetra_Map

    \author u.kue
    \date 05/07
   */
  void AllreduceEMap(vector<int>& rredundant, const Epetra_Map& emap);

  /// Create an allreduced gid to index map from the given Epetra_Map
  /*!
    We have nodes and elements with unique but otherwise arbitrary
    global ids. But unfortunately we need an allreduced vector of dof
    numbers during the dof assignment phase. In order to use such a
    vector we need to map from global ids to vector indexes. Here we
    provide that map.

    \note You are not supposed to use redundant vectors in normal
    situations. If you happen to need this method you are probably
    about to do something illegal.

    \param idxmap (o) map from global ids to (redundant) vector indexes
    \param emap (i) distributed Epetra_Map

    \author u.kue
    \date 05/07
   */
  void AllreduceEMap(map<int,int>& idxmap, const Epetra_Map& emap);

  /// Create an allreduced gid to index map from the given Epetra_Map
  /// on a distinct processor, all other procs create empty maps instead.
  /*!
    This method is currently used within the parallel post_drt_ensight
    filter in order to import all values stored in a distributed Epetra_Vector
    to processor 0 for writing them into file.

    \note see also documentation for the usual AllreduceEMap methods

    \param emap (i) any distributed Epetra_Map
    \param pid (i)  processor id where you want to have the allreduced map
    				exclusively
    \author gjb
    \date 11/07
   */
  RCP<Epetra_Map> AllreduceEMap(const Epetra_Map& emap, const int pid);

  /// Create an allreduced Epetra_Map from the given Epetra_Map
  /// and give it to all processors.
  /*!
    This method is currently used within the constraint management, since
    current values of constraint values and langrange multipliers are distributed
    uniquely for computation. At some places we need the full information of these
    values on every processor, so this method has to be used.

    \note You are not supposed to use redundant vectors in normal
    situations. If you happen to need this method you are probably
    about to do something illegal.

    \param emap (i) any distributed Epetra_Map

    \author tk
    \date 04/08
   */
  RCP<Epetra_Map> AllreduceEMap(const Epetra_Map& emap);

  /// find position of my map elements in a consecutive vector
  /*!
    The idea is to put the entries of a given map into a redundant
    vector, ordered by processor number. The map is assumed to be
    nonoverlapping. Here we figure out the index of our first entry in
    that vector.

    \note You are not supposed to use redundant vectors in normal
    situations. If you happen to need this method you are probably
    about to do something illegal.

    \param nummyelements (i) number of elements on this proc
    \param comm (i) communicator

    \return vector position of first entry on each processor

    \author u.kue
    \date 05/07
   */
  int FindMyPos(int nummyelements, const Epetra_Comm& comm);

  /// Communication between all pairs of processes, with distinct data for each.
  /*!
    Sends a different vector<int> to each processes. The size of each vector may
    be different, zero-length vectors are allowed.
    Communication is implemented with the MPI function MPI_Alltoallv.

    \param comm (i) communicator
    \param send (i) vector of length comm.NumProc(), j-th element to be send to
    j-th processor.
    \param recv (o) vector of length comm.NumProc(), j-th element received from
    j-th processor.

    \author h.kue
    \date 09/07
   */
  void AllToAllCommunication( const Epetra_Comm& comm,
                              const vector< vector<int> >& send, vector< vector<int> >& recv );

  /// return the first slot of a pair
  /*!
    To be used with stl algorithms.

    This should be part of stl but is not. So we define our own version.
  */
  template <typename pair_type>
  struct select1st: public std::unary_function<const pair_type &,
                                               const typename pair_type::first_type &>
  {
    const typename pair_type::first_type &operator()(const pair_type &v) const
    {
      return v.first;
    }
  };


  /// return the second slot of a pair
  /*!
    To be used with stl algorithms.

    This should be part of stl but is not. So we define our own version.
  */
  template <typename pair_type>
  struct select2nd: public std::unary_function<const pair_type &,
                                               const typename pair_type::second_type &>
  {
    const typename pair_type::second_type &operator()(const pair_type &v) const
    {
      return v.second;
    }
  };


  /// mem_fun_t version from gcc adapted to Teuchos::RCP
  /*!
    \note This is an internal class you never use directly. Use the rcp_fun
    method instead.
   */
  template <class _Ret, class _Tp>
  class rcp_fun_t : public unary_function<const _Tp*, _Ret>
  {
  public:
    explicit rcp_fun_t(_Ret (_Tp::*__pf)()) : _M_f(__pf) {}

    _Ret operator()(const Teuchos::RCP<_Tp> p) const { return ((&*p)->*_M_f)(); }
  private:
    _Ret (_Tp::*_M_f)();
  };


  /// mem_fun_t version from gcc adapted to Teuchos::RCP
  /*!
    \note This is an internal class you never use directly. Use the rcp_fun
    method instead.
   */
  template <class _Ret, class _Tp>
  class const_rcp_fun_t : public unary_function<const _Tp*, _Ret>
  {
  public:
    explicit const_rcp_fun_t(_Ret (_Tp::*__pf)() const) : _M_f(__pf) {}

    _Ret operator()(const Teuchos::RCP<_Tp> p) const { return ((&*p)->*_M_f)(); }
  private:
    _Ret (_Tp::*_M_f)() const;
  };


  /// mem_fun version from gcc adapted to Teuchos::RCP
  /*!
    This is needed if you want to use std containers (vector, set, list, ...)
    that contain RCPed objects with std algorithms. Use it in place of the
    usual std::mem_fun method.
   */
  template <class _Ret, class _Tp>
  inline rcp_fun_t<_Ret, _Tp> rcp_fun(_Ret (_Tp::*__f)()) { return rcp_fun_t<_Ret, _Tp>(__f); }

  /// mem_fun version from gcc adapted to Teuchos::RCP
  /*!
    This is needed if you want to use std containers (vector, set, list, ...)
    that contain RCPed objects with std algorithms. Use it in place of the
    usual std::mem_fun method.
   */
  template <class _Ret, class _Tp>
  inline const_rcp_fun_t<_Ret, _Tp> rcp_fun(_Ret (_Tp::*__f)() const) { return const_rcp_fun_t<_Ret, _Tp>(__f); }


} // namespace LINALG


#endif  // #ifndef LINALG_UTILS_H
#endif  // #ifdef CCADISCRET
